{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Pre-trained Models\n",
    "\n",
    "This example shows how to **customize your own pre-trained model** for new ideas. Tailor and integrate *any* **add-in** extra module within the vast pre-trained model **with lightning speed**.\n",
    "\n",
    "![tutorials_overview](./assests/tutorials_overview.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce the Custom Model\n",
    "\n",
    "Let's begin with a three-layer Multilayer Perceptron (MLP).\n",
    "\n",
    "Although a multi-layer perceptron is not a good image learner, we can quickly get started with it. For other custom networks, we can also make similar designs and modifications by analogy. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Run the code block below to customize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP Class\n",
    "    ==============\n",
    "\n",
    "    Multilayer Perceptron (MLP) model for image (224x224) classification tasks.\n",
    "    \n",
    "    Args:\n",
    "        args (object): Custom arguments or configurations.\n",
    "        num_classes (int): Number of output classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.args = args\n",
    "        self.image_size = 224\n",
    "        self.fc1 = nn.Linear(self.image_size * self.image_size * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits.\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Custom Multilayer Perceptron (MLP) Architecture](./assests/tutorials_mlp.png)\n",
    "*Figure 1: Custom Multilayer Perceptron (MLP) Architecture*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, expand models from **fleeting moments of inspiration**.\n",
    "\n",
    "We will customize and modify the network structure through a few lines of code from **ZhiJian**.\n",
    "\n",
    "The additional auxiliary structures are also implemented based on the PyTorch framework. The auxiliary structures inherit the base class `AddinBase`, which integrates some basic methods for data access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Additional Add-in Modules\n",
    "\n",
    "+ Run the code block below to customize add-in modules and entry points for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhijian.models.addin.module.base import AddinBase\n",
    "class MLPAddin(AddinBase):\n",
    "    \"\"\"\n",
    "    MLPAddin Class\n",
    "    ==============\n",
    "\n",
    "    Multilayer Perceptron (MLP) add-in.\n",
    "\n",
    "    Args:\n",
    "        config (object): Custom configuration or arguments.\n",
    "        model_config (object): Configuration specific to the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, model_config):\n",
    "        super(MLPAddin, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.embed_dim = model_config.hidden_size\n",
    "\n",
    "        self.reduction_dim = 16\n",
    "\n",
    "        self.fc1 = nn.Linear(self.embed_dim, self.reduction_dim)\n",
    "        if config.mlp_addin_output_size is not None:\n",
    "            self.fc2 = nn.Linear(self.reduction_dim, config.mlp_addin_output_size)\n",
    "        else:\n",
    "            self.fc2 = nn.Linear(self.reduction_dim, self.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the MLP add-in.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            tensor: Output tensor after passing through the MLP add-in.\n",
    "        \"\"\"\n",
    "        identity = x \n",
    "        out = self.fc1(identity)\n",
    "        out = nn.ReLU()(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def adapt_input(self, module, inputs):\n",
    "        \"\"\"\n",
    "        Hook function to adapt the input data before it enters the module.\n",
    "\n",
    "        Args:\n",
    "            module (nn.Module): The module being hooked.\n",
    "            inputs (tuple): (Inputs before the module,).\n",
    "\n",
    "        Returns:\n",
    "            tensor: Adapted input tensor after passing through the MLP add-in.\n",
    "        \"\"\"\n",
    "        x = inputs[0]\n",
    "        return self.forward(x)\n",
    "\n",
    "    def adapt_output(self, module, inputs, outputs):\n",
    "        \"\"\"\n",
    "        Hook function to adapt the output data after it leaves the module.\n",
    "\n",
    "        Args:\n",
    "            module (nn.Module): The module being hooked.\n",
    "            inputs (tuple): (Inputs before the module,).\n",
    "            outputs (tensor): Outputs after the module.\n",
    "\n",
    "        Returns:\n",
    "            tensor: Adapted output tensor after passing through the MLP add-in.\n",
    "        \"\"\"\n",
    "        return self.forward(outputs)\n",
    "    \n",
    "    def adapt_across_input(self, module, inputs):\n",
    "        \"\"\"\n",
    "        Hook function to adapt the data across the modules.\n",
    "\n",
    "        Args:\n",
    "            module (nn.Module): The module being hooked.\n",
    "            inputs (tuple): (Inputs before the module,).\n",
    "\n",
    "        Returns:\n",
    "            tensor: Adapted input tensor after adding the MLP add-in output to the subsequent module.\n",
    "        \"\"\"\n",
    "        x = inputs[0]\n",
    "        x = x + self.forward(self.inputs_cache)\n",
    "        return x\n",
    "\n",
    "    def adapt_across_output(self, module, inputs, outputs):\n",
    "        \"\"\"\n",
    "        Hook function to adapt the data across the modules.\n",
    "\n",
    "        Args:\n",
    "            module (nn.Module): The module being hooked.\n",
    "            inputs (tuple): (Inputs before the module,).\n",
    "            outputs (tensor): Outputs after the module.\n",
    "\n",
    "        Returns:\n",
    "            tensor: Adapted input tensor after adding the MLP add-in output to the previous module.\n",
    "        \"\"\"\n",
    "        outputs = outputs + self.forward(self.inputs_cache)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the extended auxiliary structure `MLPAddin` mentioned above, we add a low-rank bottleneck (consisting of two linear layers, with a reduced dimension in the middle) inspired by efficient parameter methods like *Adapter* or *LoRA*. We define and implement this in the `__init__` and `forward` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Additional Auxiliary Structure Example](./assests/tutorials_addin_structure.png)\n",
    "*Figure 2: Additional Auxiliary Structure Example*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the `hook` methods starting with `adapt_` are our entry functions. They serve as hooks to attach the extended modules to the base model. We will further explain their roles in the following text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Inter-layer Insertion & Cross-layer Concatenation Points\n",
    "\n",
    "We aim to customize our model by **inter-layer insertion** and **cross-layer concatenation** of the auxiliary structures at different positions within the base model (such as the custom MLP mentioned earlier). When configuring the insertion or concatenation positions, **ZhiJian** provides **a minimalistic one-line configuration syntax**.\n",
    "\n",
    "The syntax for configuring add-in module into the base model is as follows. We will start with one or two examples and gradually understand the meaning of each configuration part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ *Inter-layer Insertion*:\n",
    "\n",
    "  ```python\n",
    "  >>> (MLPAddin.adapt_input): ...->{inout1}(fc2)->...\n",
    "  ```\n",
    "\n",
    "  ![Additional Add-in Structure - Inter-layer Insertion 1](./assests/tutorials_mlp_addin_1.png)\n",
    "  *Figure 3: Additional Add-in Structure - Inter-layer Insertion 1*\n",
    "\n",
    "\n",
    "  ```python\n",
    "  >>> (MLPAddin.adapt_input): ...->{inout1}(fc2)->...\n",
    "  ```\n",
    "\n",
    "\n",
    "  ![Additional Add-in Structure - Inter-layer Insertion 2](./assests/tutorials_mlp_addin_2.png)\n",
    "  *Figure 4: Additional Add-in Structure - Inter-layer Insertion 2*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ *Cross-layer Insertion*:\n",
    "\n",
    "  ```python\n",
    "  >>> (MLPAddin.adapt_across_input): ...->(fc1){in1}->...->{out1}(fc3)->...\n",
    "  ```\n",
    "\n",
    "  ![Additional Add-in Structure - Inter-layer Insertion 3](./assests/tutorials_mlp_addin_3.png)\n",
    "  *Figure 5: Additional Add-in Structure - Cross-layer Concatenation*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Module: `->(fc1)`\n",
    "\n",
    "Consider a base model implemented based on the PyTorch framework, where the representation of each layer and module in the model is straightforward：\n",
    "\n",
    "\n",
    "+ As shown in the figure, the print command can output the defined names of the model structure:\n",
    "\n",
    "  ```python\n",
    "  $ print(model)\n",
    "  ```\n",
    "\n",
    "+ The structure of some classic backbone can be represented as follows\n",
    "\n",
    "\n",
    "  + MLP:\n",
    "\n",
    "    ```python\n",
    "    >>> input->(fc1)->(fc2)->(fc3)->output\n",
    "    ```\n",
    "\n",
    "  + ViT `block[i]`:\n",
    "  \n",
    "    ```python\n",
    "    >>> input->...->(block[i].norm1)->\n",
    "            (block[i].attn.qkv)->(block[i].attn.attn_drop)->(block[i].attn.proj)->(block[i].attn.proj_drop)->\n",
    "            (block[i].ls1)->(block[i].drop_path1)->\n",
    "                (block[i].norm2)->\n",
    "                (block[i].mlp.fc1)->(block[i].mlp.act)->(block[i].mlp.drop1)->(block[i].mlp.fc2)->(block[i].mlp.drop2)->\n",
    "                    (block[i].ls2)->(block[i].drop_path2)->...->output\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Module: `...`\n",
    "\n",
    "In the configuration syntax of **ZhiJian**, the `...` can be used to represent the default layer or module.\n",
    "\n",
    "+ For example, when we only focus on the `(fc2)` module in MLP and the `(block[i].mlp.fc2)` module in ViT:\n",
    "\n",
    "  + MLP:\n",
    "\n",
    "    ```python\n",
    "    >>> ...->(fc2)->...\n",
    "    ```\n",
    "  + ViT:\n",
    "  \n",
    "    ```python\n",
    "    >>> ...->(block[i].mlp.fc2)->...\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insertion & Concatenation Function: `():`\n",
    "\n",
    "Considering the custom auxiliary structure `MLPAddin` mentioned above, the functions starting with `adapt_` will serve as the processing center that **insert** and **concatenate** into the base model.\n",
    "\n",
    "\n",
    "+ There are primarily two types of parameter passing methods:\n",
    "\n",
    "  ```python\n",
    "  def adapt_input(self, module, inputs):\n",
    "      \"\"\"\n",
    "      Args:\n",
    "          module (nn.Module): The module being hooked.\n",
    "          inputs (tuple): (Inputs before the module,).\n",
    "      \"\"\"\n",
    "      ...\n",
    "  \n",
    "  def adapt_output(self, module, inputs, outputs):\n",
    "      \"\"\"\n",
    "      Args:\n",
    "          module (nn.Module): The module being hooked.\n",
    "          inputs (tuple): (Inputs before the module,).\n",
    "          outputs (tensor): Outputs after the module.\n",
    "      \"\"\"\n",
    "      ...\n",
    "  ```\n",
    "\n",
    "  where\n",
    "\n",
    "  + `adapt_input(self, module, inputs)` is generally set before the module and is called before the data enters the module to process inputs and truncate the `input`.\n",
    "\n",
    "  + `adapt_output(self, module, inputs, outputs)` is generally set before the module and is called before the data enters the module to process outputs and truncate the `output`.\n",
    "\n",
    "These functions will be \"hooked\" into the base model in the main method of configuring the module, serving as key connectors between the base model and the auxiliary structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insertion & Concatenation Point: `{}`\n",
    "\n",
    "Consider an independent extended auxiliary structure (such as the `MLPAddin` mentioned above), its **insertion or concatenation points** with the base network must consist of *\"Data Input\"* and *\"Data Output\"* where:\n",
    "\n",
    "+ **\"Data Input\"** refers to the network features input into the extended auxiliary structure.\n",
    "+ **\"Data Output\"** refers to the adapted features output from the auxiliary structure back to the base network.\n",
    "\n",
    "\n",
    "Next, let's use some configuration examples of MLP to illustrate the syntax and functionality of **ZhiJian** for **module integration**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inter-layer Insertion: `inout`\n",
    "\n",
    "+ As shown in the above figure, the configuration expression is:\n",
    "\n",
    "  ```python\n",
    "  >>> (MLPAddin.adapt_input): ...->{inout1}(fc2)->...\n",
    "  ```\n",
    "\n",
    "  where\n",
    "\n",
    "  + `{inout1}` refers to the position which gets the base model features (or output, at any layer or module).\n",
    "  \n",
    "    It denotes the *\"Data Input\"* and *\"Data Output\"*. The configuration can be `{inoutx}`, where `x` represents the x<sup>th</sup> integration point. For example, `{inout1}` represents the first integration point.\n",
    "\n",
    "  + In the example above, this inter-layer insertion configuration *truncates* the features of the input `fc2` module, *passes* them through, and then return to the `fc2` module. At this point, the original `fc2` features no longer enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-layer Concatenation `in`, `out`\n",
    "\n",
    "+ As shown in the above figure, the configuration expression is:\n",
    "\n",
    "  ```python\n",
    "  >>> (MLPAddin.adapt_across_input): ...->(fc1){in1}->...->{out1}(fc3)->...\n",
    "  ```\n",
    "\n",
    "  where\n",
    "\n",
    "  + `{in1}`: represents the integration point where the base network features (or output, at any layer or module) *enter* the additional add-in structure.\n",
    "  \n",
    "    It denotes the *\"Data Input\"*. The configuration can be `{inx}`, where `x` represents the x\\ :sup:`th` integration point. For example, `{in1}` represents the first integration point.\n",
    "\n",
    "  + `{out1}`: represent the integration points where the features processed by the additional add-in structure are *returned* to the base network.\n",
    "\n",
    "    It denotes the *\"Data Output\"*. The configuration can be `{outx}`, where `x` represents the x\\ :sup:`th` integration point. For example, `{out1}` represents the first integration point.\n",
    "    \n",
    "  + This cross-layer concatenation configuration *extracts* the features of the `fc1` module's output, *passes them into* the auxiliary structure, and then *returns* them to the base network before the `fc3` module in the form of residual addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ For a better prompt, let's create a tool function that guides the input first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available add-in structure(s):\n",
      "\t[1] Insert between `fc1` and `fc2` layer (performed before `fc2`)\n",
      "\t[2] Insert between `fc1` and `fc2` layer (performed after `fc1`)\n",
      "\t[3] Splice across `fc2` layer (performed before `fc2` and `fc3`)\n",
      "\t[4] Splice across `fc2` layer (performed after `fc1` and before `fc3`)\n",
      "\t[5] Splice across `fc2` layer (performed before and after `fc2`)\n",
      "\t[6] Splice across `fc2` layer (performed after `fc1` and `fc2`)\n",
      "Your selection: Splice across `fc2` layer (performed before and after `fc2`)\n"
     ]
    }
   ],
   "source": [
    "def select_from_input(prompt_for_select, valid_selections):\n",
    "    selections2print = '\\n\\t'.join([f'[{idx + 1}] {i}' for idx, i in enumerate(valid_selections)])\n",
    "    while True:\n",
    "        selected = input(f\"Please input a {prompt_for_select}, type 'help' to show the options: \")\n",
    "\n",
    "        if selected == 'help':\n",
    "            print(f\"Available {prompt_for_select}(s):\\n\\t{selections2print}\")\n",
    "        elif selected.isdigit() and int(selected) >= 1 and int(selected) <= len(valid_selections):\n",
    "            selected = valid_selections[int(selected) - 1]\n",
    "            break\n",
    "        elif selected in valid_selections:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Sorry, input not support.\")\n",
    "            print(f\"Available {prompt_for_select}(s):\\n\\t{selections2print}\")\n",
    "\n",
    "    print(f\"Your selection: {selected}\")\n",
    "    return selected\n",
    "\n",
    "available_example_config_blitzs = {\n",
    "    'Insert between `fc1` and `fc2` layer (performed before `fc2`)': \"(MLPAddin.adapt_input): ...->{inout1}(fc2)->...\",\n",
    "    'Insert between `fc1` and `fc2` layer (performed after `fc1`)': \"(MLPAddin.adapt_output): ...->(fc1){inout1}->...\",\n",
    "    'Splice across `fc2` layer (performed before `fc2` and `fc3`)': \"(MLPAddin.adapt_across_input): ...->{in1}(fc2)->{out1}(fc3)->...\",\n",
    "    'Splice across `fc2` layer (performed after `fc1` and before `fc3`)': \"(MLPAddin.adapt_across_input): ...->(fc1){in1}->...->{out1}(fc3)->...\",\n",
    "    'Splice across `fc2` layer (performed before and after `fc2`)': \"(MLPAddin.adapt_across_output): ...->{in1}(fc2){out1}->...\",\n",
    "    'Splice across `fc2` layer (performed after `fc1` and `fc2`)': \"(MLPAddin.adapt_across_output): ...->(fc1){in1}->(fc2){out1}->...\",\n",
    "}\n",
    "\n",
    "config_blitz = available_example_config_blitzs[select_from_input('add-in structure', list(available_example_config_blitzs.keys()))] # user input about model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available reuse modules(s):\n",
      "\t[1] add-ins and linear layer\n",
      "\t[2] add-ins and the last layer of feature extractor and the linear layer (Partial-1)\n",
      "Your selection: add-ins and linear layer\n"
     ]
    }
   ],
   "source": [
    "available_example_reuse_modules = {\n",
    "    'timm.vit_base_patch16_224_in21k': {\n",
    "        'add-ins and linear layer': 'addin,fc3',\n",
    "        'add-ins and the last layer of feature extractor and the linear layer (Partial-1)': 'addin,fc2,fc3',\n",
    "    }\n",
    "}\n",
    "\n",
    "availables       = available_example_reuse_modules['timm.vit_base_patch16_224_in21k']\n",
    "reuse_keys_blitz = availables[select_from_input('reuse modules', list(availables.keys()))] # user input about reuse modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available dataset(s):\n",
      "\t[1] VTAB-1k.CIFAR-100\n",
      "\t[2] VTAB-1k.CLEVR-Count\n",
      "\t[3] VTAB-1k.CLEVR-Distance\n",
      "\t[4] VTAB-1k.Caltech101\n",
      "\t[5] VTAB-1k.DTD\n",
      "\t[6] VTAB-1k.Diabetic-Retinopathy\n",
      "\t[7] VTAB-1k.Dmlab\n",
      "\t[8] VTAB-1k.EuroSAT\n",
      "\t[9] VTAB-1k.KITTI\n",
      "\t[10] VTAB-1k.Oxford-Flowers-102\n",
      "\t[11] VTAB-1k.Oxford-IIIT-Pet\n",
      "\t[12] VTAB-1k.PatchCamelyon\n",
      "\t[13] VTAB-1k.RESISC45\n",
      "\t[14] VTAB-1k.SUN397\n",
      "\t[15] VTAB-1k.SVHN\n",
      "\t[16] VTAB-1k.dSprites-Location\n",
      "\t[17] VTAB-1k.dSprites-Orientation\n",
      "\t[18] VTAB-1k.smallNORB-Azimuth\n",
      "\t[19] VTAB-1k.smallNORB-Elevation\n",
      "Your selection: VTAB-1k.CIFAR-100\n",
      "Your dataset directory: /data/zhangyk/data/zhijian\n"
     ]
    }
   ],
   "source": [
    "available_datasets = [\n",
    "    'VTAB-1k.CIFAR-100', 'VTAB-1k.CLEVR-Count', 'VTAB-1k.CLEVR-Distance', 'VTAB-1k.Caltech101', 'VTAB-1k.DTD',\n",
    "    'VTAB-1k.Diabetic-Retinopathy', 'VTAB-1k.Dmlab', 'VTAB-1k.EuroSAT', 'VTAB-1k.KITTI', 'VTAB-1k.Oxford-Flowers-102',\n",
    "    'VTAB-1k.Oxford-IIIT-Pet', 'VTAB-1k.PatchCamelyon', 'VTAB-1k.RESISC45', 'VTAB-1k.SUN397', 'VTAB-1k.SVHN',\n",
    "    'VTAB-1k.dSprites-Location', 'VTAB-1k.dSprites-Orientation', 'VTAB-1k.smallNORB-Azimuth', 'VTAB-1k.smallNORB-Elevation'\n",
    "] # dataset options.\n",
    "dataset = select_from_input('dataset', available_datasets)  # user input about dataset\n",
    "dataset_dir = input(f\"Please input your dataset directory: \")   # user input about dataset directory\n",
    "print(f\"Your dataset directory: {dataset_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Next, we will configure the parameters and proceed with model training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aa': None,\n",
      " 'addins': [{'hook': [['get_pre', 'pre'], ['adapt_across_output', 'post']],\n",
      "             'location': [['fc2'], ['fc2']],\n",
      "             'name': 'MLPAddin'}],\n",
      " 'amp': False,\n",
      " 'amp_dtype': 'float16',\n",
      " 'amp_impl': 'native',\n",
      " 'aot_autograd': False,\n",
      " 'aug_repeats': 0,\n",
      " 'aug_splits': 0,\n",
      " 'batch_size': 64,\n",
      " 'bce_loss': False,\n",
      " 'bce_target_thresh': None,\n",
      " 'bn_eps': None,\n",
      " 'bn_momentum': None,\n",
      " 'channels_last': False,\n",
      " 'checkpoint_hist': 10,\n",
      " 'class_map': '',\n",
      " 'clip_grad': None,\n",
      " 'clip_mode': 'norm',\n",
      " 'color_jitter': 0.4,\n",
      " 'config_blitz': '(MLPAddin.adapt_across_output): ...->{in1}(fc2){out1}->...',\n",
      " 'cooldown_epochs': 0,\n",
      " 'crop_mode': None,\n",
      " 'crop_pct': None,\n",
      " 'cutmix': 0.0,\n",
      " 'cutmix_minmax': None,\n",
      " 'data': None,\n",
      " 'data_dir': None,\n",
      " 'dataset': 'VTAB-1k.CIFAR-100',\n",
      " 'dataset_dir': '/data/zhangyk/data/zhijian',\n",
      " 'dataset_download': False,\n",
      " 'decay_epochs': 90,\n",
      " 'decay_milestones': [90, 180, 270],\n",
      " 'decay_rate': 0.1,\n",
      " 'dist_bn': 'reduce',\n",
      " 'drop': 0.0,\n",
      " 'drop_block': None,\n",
      " 'drop_connect': None,\n",
      " 'drop_path': None,\n",
      " 'epoch_repeats': 0.0,\n",
      " 'epochs': 300,\n",
      " 'eta_min': 0,\n",
      " 'eval_metric': 'top1',\n",
      " 'experiment': '',\n",
      " 'fast_norm': False,\n",
      " 'fuser': '',\n",
      " 'gp': None,\n",
      " 'gpu': '0',\n",
      " 'grad_checkpointing': False,\n",
      " 'hflip': 0.5,\n",
      " 'img_size': None,\n",
      " 'in_chans': None,\n",
      " 'initial_checkpoint': '',\n",
      " 'input_size': None,\n",
      " 'interpolation': '',\n",
      " 'jsd_loss': False,\n",
      " 'layer_decay': None,\n",
      " 'local_rank': 0,\n",
      " 'log_url': 'your/log/directory',\n",
      " 'log_wandb': False,\n",
      " 'lr': 0.001,\n",
      " 'lr_base': 0.1,\n",
      " 'lr_base_scale': '',\n",
      " 'lr_base_size': 256,\n",
      " 'lr_cycle_decay': 0.5,\n",
      " 'lr_cycle_limit': 1,\n",
      " 'lr_cycle_mul': 1.0,\n",
      " 'lr_k_decay': 1.0,\n",
      " 'lr_noise': None,\n",
      " 'lr_noise_pct': 0.67,\n",
      " 'lr_noise_std': 1.0,\n",
      " 'max_epoch': 5,\n",
      " 'mean': None,\n",
      " 'min_lr': 0,\n",
      " 'mixup': 0.0,\n",
      " 'mixup_mode': 'batch',\n",
      " 'mixup_off_epoch': 0,\n",
      " 'mixup_prob': 1.0,\n",
      " 'mixup_switch_prob': 0.5,\n",
      " 'model': 'timm.vit_base_patch16_224_in21k',\n",
      " 'model_ema': False,\n",
      " 'model_ema_decay': 0.9998,\n",
      " 'model_ema_force_cpu': False,\n",
      " 'momentum': 0.9,\n",
      " 'no_aug': False,\n",
      " 'no_ddp_bb': False,\n",
      " 'no_prefetcher': False,\n",
      " 'no_resume_opt': False,\n",
      " 'num_classes': None,\n",
      " 'num_workers': 8,\n",
      " 'only_do_test': False,\n",
      " 'opt': 'sgd',\n",
      " 'opt_betas': None,\n",
      " 'opt_eps': None,\n",
      " 'optimizer': 'adam',\n",
      " 'output': '',\n",
      " 'patience_epochs': 10,\n",
      " 'pin_mem': False,\n",
      " 'pretrained': False,\n",
      " 'pretrained_url': [],\n",
      " 'ratio': [0.75, 1.3333333333333333],\n",
      " 'recount': 1,\n",
      " 'recovery_interval': 0,\n",
      " 'remode': 'pixel',\n",
      " 'reprob': 0.0,\n",
      " 'resplit': False,\n",
      " 'resume': '',\n",
      " 'reuse_keys': [['addin'], ['fc3']],\n",
      " 'reuse_keys_blitz': 'addin,fc3',\n",
      " 'save_images': False,\n",
      " 'scale': [0.08, 1.0],\n",
      " 'sched': 'cosine',\n",
      " 'sched_on_updates': False,\n",
      " 'seed': 0,\n",
      " 'smoothing': 0.1,\n",
      " 'split_bn': False,\n",
      " 'start_epoch': None,\n",
      " 'std': None,\n",
      " 'sync_bn': False,\n",
      " 'time_str': '0718-19-52-36-748',\n",
      " 'torchcompile': None,\n",
      " 'torchscript': False,\n",
      " 'train_interpolation': 'random',\n",
      " 'train_split': 'train',\n",
      " 'training_mode': 'finetune',\n",
      " 'tta': 0,\n",
      " 'use_multi_epochs_loader': False,\n",
      " 'val_split': 'validation',\n",
      " 'validation_batch_size': None,\n",
      " 'verbose': True,\n",
      " 'vflip': 0.0,\n",
      " 'warmup_epochs': 5,\n",
      " 'warmup_lr': 1e-05,\n",
      " 'warmup_prefix': False,\n",
      " 'wd': 5e-05,\n",
      " 'weight_decay': 2e-05,\n",
      " 'worker_seeding': 'all'}\n"
     ]
    }
   ],
   "source": [
    "from zhijian.trainers.base import prepare_args\n",
    "from zhijian.models.utils import pprint, dict2args\n",
    "training_mode = 'finetune'\n",
    "args = dict2args({\n",
    "    'log_url': 'your/log/directory',             # log directory\n",
    "    'model': 'timm.vit_base_patch16_224_in21k',  # backbone network\n",
    "    'config_blitz': config_blitz,                # addin blitz configuration\n",
    "    'dataset': dataset,                          # dataset\n",
    "    'dataset_dir': dataset_dir,                  # dataset directory\n",
    "    'training_mode': training_mode,              # training mode\n",
    "    'reuse_keys_blitz': reuse_keys_blitz,        # reuse keys blitz configuration\n",
    "    'optimizer': 'adam',                         # optimizer\n",
    "    'batch_size': 64,                            # batch size\n",
    "    'num_workers': 8,                            # num workers\n",
    "    'max_epoch': 5,                              # max epoch\n",
    "    'eta_min': 0,                                # eta_min of CosineAnnealingLR\n",
    "    'lr': 1e-3,                                  # learning rate\n",
    "    'wd': 5e-5,                                  # weight decay\n",
    "    'gpu': '0',                                  # gpu id\n",
    "    'seed': 0,                                   # random seed\n",
    "    'verbose': True,                             # control the verbosity of the output\n",
    "    'only_do_test': False                        # test flag\n",
    "})      \n",
    "\n",
    "args = prepare_args(args, update_default=True)\n",
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Run the code block below to configure the GPU and the model (excluding additional auxiliary structures):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "assert torch.cuda.is_available()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "torch.cuda.set_device(int(args.gpu))\n",
    "\n",
    "from zhijian.data.config import DATASET2NUM_CLASSES\n",
    "from zhijian.models.backbone.base import ModelWrapper\n",
    "from zhijian.models.configs.base import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "model       = MLP(args, DATASET2NUM_CLASSES[args.dataset.replace('VTAB-1k.','')])\n",
    "model       = ModelWrapper(model)\n",
    "model_args  = dict2args({'hidden_size': 256, 'input_size': (224, 224), 'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Run the code block below to configure additional auxiliary structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhijian.models.addin.base import prepare_addins\n",
    "from zhijian.models.backbone.base import prepare_hook, prepare_gradient, prepare_cuda\n",
    "args.mlp_addin_output_size = 256\n",
    "addins, fixed_params = prepare_addins(args, model_args, addin_classes=[MLPAddin])\n",
    "\n",
    "prepare_hook(args.addins, addins, model, 'addin')\n",
    "prepare_gradient(args.reuse_keys, model)\n",
    "device = prepare_cuda(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Run the code block below to configure the dataset, optimizer, loss function, and other settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhijian.data.base import prepare_vision_dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader, val_loader, num_classes = prepare_vision_dataloader(args, model_args)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=args.lr,\n",
    "    weight_decay=args.wd\n",
    "    )\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    args.max_epoch,\n",
    "    eta_min=args.eta_min\n",
    "    )\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Run the code block below to prepare the `trainer` object and start training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log level set to: INFO\n",
      "Log files are recorded in: your/log/directory/0718-19-52-36-748\n",
      "Trainable/total parameters of the model: 0.03M / 38.64M (0.08843%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch   GPU Mem.       Time       Loss         LR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5     0.589G     0.1355      4.602      0.001: 100%|██████████| 16.0/16.0 [00:01<00:00, 12.9batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch   GPU Mem.       Time      Acc@1      Acc@5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5     0.629G    0.03114      1.871      7.932: 100%|██████████| 157/157 [00:05<00:00, 30.9batch/s] \n",
      "***   Best results: [Acc@1: 1.8710191082802548], [Acc@5: 7.931926751592357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch   GPU Mem.       Time       Loss         LR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5     0.784G     0.1016      4.538 0.00090451: 100%|██████████| 16.0/16.0 [00:00<00:00, 19.4batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch   GPU Mem.       Time      Acc@1      Acc@5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5     0.784G    0.02669      2.498      9.504: 100%|██████████| 157/157 [00:04<00:00, 35.9batch/s] \n",
      "***   Best results: [Acc@1: 2.4980095541401273], [Acc@5: 9.504378980891719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch   GPU Mem.       Time       Loss         LR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5     0.784G    0.09631      4.488 0.00065451: 100%|██████████| 16.0/16.0 [00:00<00:00, 20.6batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch   GPU Mem.       Time      Acc@1      Acc@5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5     0.784G    0.02688      2.379      10.16: 100%|██████████| 157/157 [00:04<00:00, 36.0batch/s] \n",
      "***   Best results: [Acc@1: 2.3785828025477707], [Acc@5: 10.161226114649681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch   GPU Mem.       Time       Loss         LR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5     0.784G    0.09126       4.45 0.00034549: 100%|██████████| 16.0/16.0 [00:00<00:00, 20.2batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch   GPU Mem.       Time      Acc@1      Acc@5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5     0.784G    0.02644      2.468      10.29: 100%|██████████| 157/157 [00:04<00:00, 36.2batch/s] \n",
      "***   Best results: [Acc@1: 2.468152866242038], [Acc@5: 10.290605095541402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch   GPU Mem.       Time       Loss         LR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5     0.784G     0.0936      4.431 9.5492e-05: 100%|██████████| 16.0/16.0 [00:00<00:00, 20.5batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch   GPU Mem.       Time      Acc@1      Acc@5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5     0.784G    0.02706      2.558      10.43: 100%|██████████| 157/157 [00:04<00:00, 35.8batch/s] \n",
      "***   Best results: [Acc@1: 2.557722929936306], [Acc@5: 10.429936305732484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch   GPU Mem.       Time      Acc@1      Acc@5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5     0.784G    0.02667      2.558      10.43: 100%|██████████| 157/157 [00:04<00:00, 36.0batch/s] \n",
      "***   Best results: [Acc@1: 2.557722929936306], [Acc@5: 10.429936305732484]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.557722929936306, 10.429936305732484)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from zhijian.trainers.base import prepare_trainer\n",
    "trainer = prepare_trainer(\n",
    "    args,\n",
    "    model=model,\n",
    "    model_args=model_args,\n",
    "    device=device,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_classes=num_classes,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    criterion=criterion\n",
    "    )\n",
    "\n",
    "trainer.fit()\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9caad242532f4f369234d325da12b156772905bc900e2fd34972bfccb470d55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
